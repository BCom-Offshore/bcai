{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64ccac3d",
   "metadata": {},
   "source": [
    "# Anomaly Detection Model Development\n",
    "## Isolation Forest for Network, Site, and Link Anomalies\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load data from PostgreSQL\n",
    "2. Engineer features for anomaly detection\n",
    "3. Train Isolation Forest models\n",
    "4. Evaluate model performance\n",
    "5. Export models to pickle for web application deployment\n",
    "\n",
    "**Note**: Update database connection details and feature columns based on your actual schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befc4cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import pickle\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import psycopg\n",
    "import json\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a502b9f",
   "metadata": {},
   "source": [
    "## Database Configuration\n",
    "\n",
    "Update these connection details to match your PostgreSQL setup:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e419ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection parameters\n",
    "# IMPORTANT: Update these with your actual database credentials\n",
    "DB_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432,\n",
    "    \"database\": \"bcom_bolt\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"your_password_here\"\n",
    "}\n",
    "\n",
    "# Path configuration for model exports\n",
    "MODELS_DIR = Path(\"../../ml_models/anomaly_detection\")\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Models will be saved to: {MODELS_DIR.absolute()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b77936",
   "metadata": {},
   "source": [
    "## Load and Prepare Data\n",
    "\n",
    "Load data from PostgreSQL and prepare features for anomaly detection.\n",
    "\n",
    "**Note**: Adjust the SQL queries based on your actual table schemas (provided by user)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e148cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_network_metrics():\n",
    "    \"\"\"\n",
    "    Load network metrics data from PostgreSQL.\n",
    "    \n",
    "    TEMPLATE: Update query based on your network_metrics table schema\n",
    "    Expected columns: [id, user_id, timestamp, latency, packet_loss, bandwidth, etc.]\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        id,\n",
    "        user_id,\n",
    "        timestamp,\n",
    "        -- Add your actual metric columns here\n",
    "        -- latency,\n",
    "        -- packet_loss,\n",
    "        -- bandwidth,\n",
    "        created_at\n",
    "    FROM network_metrics\n",
    "    WHERE created_at >= NOW() - INTERVAL '90 days'\n",
    "    ORDER BY created_at DESC\n",
    "    LIMIT 100000\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with psycopg.connect(**DB_CONFIG) as conn:\n",
    "            df = pd.read_sql(query, conn)\n",
    "        logger.info(f\"Loaded {len(df)} network metric records\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading network metrics: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_site_metrics():\n",
    "    \"\"\"\n",
    "    Load site metrics data from PostgreSQL.\n",
    "    \n",
    "    TEMPLATE: Update query based on your site_metrics table schema\n",
    "    Expected columns: [id, site_id, timestamp, response_time, error_rate, etc.]\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        id,\n",
    "        site_id,\n",
    "        timestamp,\n",
    "        -- Add your actual metric columns here\n",
    "        -- response_time,\n",
    "        -- error_rate,\n",
    "        -- uptime,\n",
    "        created_at\n",
    "    FROM site_metrics\n",
    "    WHERE created_at >= NOW() - INTERVAL '90 days'\n",
    "    ORDER BY created_at DESC\n",
    "    LIMIT 100000\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with psycopg.connect(**DB_CONFIG) as conn:\n",
    "            df = pd.read_sql(query, conn)\n",
    "        logger.info(f\"Loaded {len(df)} site metric records\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading site metrics: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_link_metrics():\n",
    "    \"\"\"\n",
    "    Load link metrics data from PostgreSQL.\n",
    "    \n",
    "    TEMPLATE: Update query based on your link_metrics table schema\n",
    "    Expected columns: [id, link_id, timestamp, jitter, throughput, etc.]\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        id,\n",
    "        link_id,\n",
    "        timestamp,\n",
    "        -- Add your actual metric columns here\n",
    "        -- jitter,\n",
    "        -- throughput,\n",
    "        -- packet_drop_rate,\n",
    "        created_at\n",
    "    FROM link_metrics\n",
    "    WHERE created_at >= NOW() - INTERVAL '90 days'\n",
    "    ORDER BY created_at DESC\n",
    "    LIMIT 100000\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with psycopg.connect(**DB_CONFIG) as conn:\n",
    "            df = pd.read_sql(query, conn)\n",
    "        logger.info(f\"Loaded {len(df)} link metric records\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading link metrics: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Load all datasets\n",
    "print(\"Loading data from PostgreSQL...\")\n",
    "network_df = load_network_metrics()\n",
    "site_df = load_site_metrics()\n",
    "link_df = load_link_metrics()\n",
    "\n",
    "print(f\"\\nData Summary:\")\n",
    "print(f\"  Network metrics: {network_df.shape if network_df is not None else 'FAILED TO LOAD'}\")\n",
    "print(f\"  Site metrics: {site_df.shape if site_df is not None else 'FAILED TO LOAD'}\")\n",
    "print(f\"  Link metrics: {link_df.shape if link_df is not None else 'FAILED TO LOAD'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a12cc50",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Prepare and engineer features for anomaly detection models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a1be1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_network_features(df):\n",
    "    \"\"\"\n",
    "    Prepare features for network anomaly detection.\n",
    "    \n",
    "    TEMPLATE: Update feature engineering based on your available metrics\n",
    "    \"\"\"\n",
    "    if df is None or len(df) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Create a copy to avoid modifying original\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # TODO: Add your feature engineering logic here\n",
    "    # Examples:\n",
    "    # - Calculate rolling averages\n",
    "    # - Compute statistical features (std, min, max)\n",
    "    # - Create interaction features\n",
    "    # - Normalize/scale features\n",
    "    \n",
    "    # Placeholder: Select numeric columns for training\n",
    "    numeric_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Remove ID and user_id columns\n",
    "    feature_cols = [col for col in numeric_cols if col not in ['id', 'user_id', 'site_id', 'link_id']]\n",
    "    \n",
    "    if not feature_cols:\n",
    "        logger.warning(\"No numeric features found for network data\")\n",
    "        return None\n",
    "    \n",
    "    logger.info(f\"Network features selected: {feature_cols}\")\n",
    "    return df_processed[feature_cols].fillna(df_processed[feature_cols].mean())\n",
    "\n",
    "def prepare_site_features(df):\n",
    "    \"\"\"\n",
    "    Prepare features for site anomaly detection.\n",
    "    \n",
    "    TEMPLATE: Update feature engineering based on your available metrics\n",
    "    \"\"\"\n",
    "    if df is None or len(df) == 0:\n",
    "        return None\n",
    "    \n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # TODO: Add your feature engineering logic here\n",
    "    numeric_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    feature_cols = [col for col in numeric_cols if col not in ['id', 'user_id', 'site_id', 'link_id']]\n",
    "    \n",
    "    if not feature_cols:\n",
    "        logger.warning(\"No numeric features found for site data\")\n",
    "        return None\n",
    "    \n",
    "    logger.info(f\"Site features selected: {feature_cols}\")\n",
    "    return df_processed[feature_cols].fillna(df_processed[feature_cols].mean())\n",
    "\n",
    "def prepare_link_features(df):\n",
    "    \"\"\"\n",
    "    Prepare features for link anomaly detection.\n",
    "    \n",
    "    TEMPLATE: Update feature engineering based on your available metrics\n",
    "    \"\"\"\n",
    "    if df is None or len(df) == 0:\n",
    "        return None\n",
    "    \n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # TODO: Add your feature engineering logic here\n",
    "    numeric_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    feature_cols = [col for col in numeric_cols if col not in ['id', 'user_id', 'site_id', 'link_id']]\n",
    "    \n",
    "    if not feature_cols:\n",
    "        logger.warning(\"No numeric features found for link data\")\n",
    "        return None\n",
    "    \n",
    "    logger.info(f\"Link features selected: {feature_cols}\")\n",
    "    return df_processed[feature_cols].fillna(df_processed[feature_cols].mean())\n",
    "\n",
    "# Prepare features\n",
    "print(\"Preparing features...\")\n",
    "network_features = prepare_network_features(network_df) if network_df is not None else None\n",
    "site_features = prepare_site_features(site_df) if site_df is not None else None\n",
    "link_features = prepare_link_features(link_df) if link_df is not None else None\n",
    "\n",
    "print(\"\\nFeature shapes:\")\n",
    "if network_features is not None:\n",
    "    print(f\"  Network: {network_features.shape}\")\n",
    "if site_features is not None:\n",
    "    print(f\"  Site: {site_features.shape}\")\n",
    "if link_features is not None:\n",
    "    print(f\"  Link: {link_features.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712b9070",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Train Isolation Forest models for anomaly detection on each metric type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab89e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_isolation_forest(X_train, contamination=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Train an Isolation Forest model.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training feature matrix\n",
    "        contamination: Proportion of outliers in the dataset (0-1)\n",
    "        random_state: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        Trained IsolationForest model\n",
    "    \"\"\"\n",
    "    model = IsolationForest(\n",
    "        contamination=contamination,\n",
    "        random_state=random_state,\n",
    "        n_estimators=100,\n",
    "        max_samples='auto',\n",
    "        max_features=1.0\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train)\n",
    "    logger.info(f\"Model trained with contamination={contamination}\")\n",
    "    return model\n",
    "\n",
    "# Train models\n",
    "models = {}\n",
    "scalers = {}\n",
    "\n",
    "print(\"Training Isolation Forest models...\")\n",
    "\n",
    "if network_features is not None and len(network_features) > 0:\n",
    "    # Scale features\n",
    "    scaler_network = StandardScaler()\n",
    "    network_scaled = scaler_network.fit_transform(network_features)\n",
    "    scalers['network'] = scaler_network\n",
    "    \n",
    "    # Train model\n",
    "    models['isolation_forest_network'] = train_isolation_forest(network_scaled)\n",
    "    print(\"✓ Network anomaly detector trained\")\n",
    "else:\n",
    "    print(\"✗ Skipped network model (no data)\")\n",
    "\n",
    "if site_features is not None and len(site_features) > 0:\n",
    "    # Scale features\n",
    "    scaler_site = StandardScaler()\n",
    "    site_scaled = scaler_site.fit_transform(site_features)\n",
    "    scalers['site'] = scaler_site\n",
    "    \n",
    "    # Train model\n",
    "    models['isolation_forest_site'] = train_isolation_forest(site_scaled)\n",
    "    print(\"✓ Site anomaly detector trained\")\n",
    "else:\n",
    "    print(\"✗ Skipped site model (no data)\")\n",
    "\n",
    "if link_features is not None and len(link_features) > 0:\n",
    "    # Scale features\n",
    "    scaler_link = StandardScaler()\n",
    "    link_scaled = scaler_link.fit_transform(link_features)\n",
    "    scalers['link'] = scaler_link\n",
    "    \n",
    "    # Train model\n",
    "    models['isolation_forest_link'] = train_isolation_forest(link_scaled)\n",
    "    print(\"✓ Link anomaly detector trained\")\n",
    "else:\n",
    "    print(\"✗ Skipped link model (no data)\")\n",
    "\n",
    "print(f\"\\n{len(models)} models trained successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffda357f",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Evaluate model performance and generate predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38459a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating models...\\n\")\n",
    "\n",
    "# Evaluate each model\n",
    "evaluation_results = {}\n",
    "\n",
    "if 'isolation_forest_network' in models:\n",
    "    model = models['isolation_forest_network']\n",
    "    scaler = scalers['network']\n",
    "    X_scaled = scaler.transform(network_features)\n",
    "    \n",
    "    predictions = model.predict(X_scaled)\n",
    "    anomaly_scores = model.score_samples(X_scaled)\n",
    "    \n",
    "    # Count anomalies\n",
    "    n_anomalies = (predictions == -1).sum()\n",
    "    \n",
    "    evaluation_results['network'] = {\n",
    "        'model': 'isolation_forest_network',\n",
    "        'total_samples': len(predictions),\n",
    "        'anomalies_detected': n_anomalies,\n",
    "        'anomaly_percentage': 100 * n_anomalies / len(predictions),\n",
    "        'min_score': float(anomaly_scores.min()),\n",
    "        'max_score': float(anomaly_scores.max()),\n",
    "        'mean_score': float(anomaly_scores.mean())\n",
    "    }\n",
    "    \n",
    "    print(\"Network Anomaly Detector:\")\n",
    "    print(f\"  Total samples: {len(predictions)}\")\n",
    "    print(f\"  Anomalies detected: {n_anomalies} ({100*n_anomalies/len(predictions):.2f}%)\")\n",
    "    print(f\"  Score range: [{anomaly_scores.min():.4f}, {anomaly_scores.max():.4f}]\")\n",
    "\n",
    "if 'isolation_forest_site' in models:\n",
    "    model = models['isolation_forest_site']\n",
    "    scaler = scalers['site']\n",
    "    X_scaled = scaler.transform(site_features)\n",
    "    \n",
    "    predictions = model.predict(X_scaled)\n",
    "    anomaly_scores = model.score_samples(X_scaled)\n",
    "    \n",
    "    n_anomalies = (predictions == -1).sum()\n",
    "    \n",
    "    evaluation_results['site'] = {\n",
    "        'model': 'isolation_forest_site',\n",
    "        'total_samples': len(predictions),\n",
    "        'anomalies_detected': n_anomalies,\n",
    "        'anomaly_percentage': 100 * n_anomalies / len(predictions),\n",
    "        'min_score': float(anomaly_scores.min()),\n",
    "        'max_score': float(anomaly_scores.max()),\n",
    "        'mean_score': float(anomaly_scores.mean())\n",
    "    }\n",
    "    \n",
    "    print(\"\\nSite Anomaly Detector:\")\n",
    "    print(f\"  Total samples: {len(predictions)}\")\n",
    "    print(f\"  Anomalies detected: {n_anomalies} ({100*n_anomalies/len(predictions):.2f}%)\")\n",
    "    print(f\"  Score range: [{anomaly_scores.min():.4f}, {anomaly_scores.max():.4f}]\")\n",
    "\n",
    "if 'isolation_forest_link' in models:\n",
    "    model = models['isolation_forest_link']\n",
    "    scaler = scalers['link']\n",
    "    X_scaled = scaler.transform(link_features)\n",
    "    \n",
    "    predictions = model.predict(X_scaled)\n",
    "    anomaly_scores = model.score_samples(X_scaled)\n",
    "    \n",
    "    n_anomalies = (predictions == -1).sum()\n",
    "    \n",
    "    evaluation_results['link'] = {\n",
    "        'model': 'isolation_forest_link',\n",
    "        'total_samples': len(predictions),\n",
    "        'anomalies_detected': n_anomalies,\n",
    "        'anomaly_percentage': 100 * n_anomalies / len(predictions),\n",
    "        'min_score': float(anomaly_scores.min()),\n",
    "        'max_score': float(anomaly_scores.max()),\n",
    "        'mean_score': float(anomaly_scores.mean())\n",
    "    }\n",
    "    \n",
    "    print(\"\\nLink Anomaly Detector:\")\n",
    "    print(f\"  Total samples: {len(predictions)}\")\n",
    "    print(f\"  Anomalies detected: {n_anomalies} ({100*n_anomalies/len(predictions):.2f}%)\")\n",
    "    print(f\"  Score range: [{anomaly_scores.min():.4f}, {anomaly_scores.max():.4f}]\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for name, results in evaluation_results.items():\n",
    "    print(f\"\\n{name.upper()}:\")\n",
    "    for key, value in results.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b45d38c",
   "metadata": {},
   "source": [
    "## Serialize Models to Pickle\n",
    "\n",
    "Export trained models and scalers to pickle format for web application deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dc941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import sys\n",
    "sys.path.insert(0, '../../')\n",
    "\n",
    "from app.services.model_management import ModelManager, ModelMetadata\n",
    "\n",
    "# Initialize model manager\n",
    "model_manager = ModelManager(str(MODELS_DIR.parent))\n",
    "\n",
    "print(\"Serializing models to pickle...\\n\")\n",
    "\n",
    "# Save models with metadata\n",
    "saved_models = []\n",
    "\n",
    "if 'isolation_forest_network' in models:\n",
    "    model = models['isolation_forest_network']\n",
    "    metadata = ModelMetadata(\n",
    "        model_name='isolation_forest_network',\n",
    "        version='1.0.0',\n",
    "        model_type='isolation_forest',\n",
    "        training_date=datetime.now().isoformat(),\n",
    "        description='Isolation Forest for detecting network metric anomalies',\n",
    "        hyperparameters={\n",
    "            'contamination': 0.1,\n",
    "            'n_estimators': 100,\n",
    "            'max_samples': 'auto',\n",
    "            'max_features': 1.0,\n",
    "            'random_state': 42\n",
    "        },\n",
    "        metrics=evaluation_results.get('network', {})\n",
    "    )\n",
    "    \n",
    "    success, path = model_manager.save_model(\n",
    "        model, 'isolation_forest_network', 'anomaly_detection', '1.0.0', metadata\n",
    "    )\n",
    "    \n",
    "    if success:\n",
    "        print(f\"✓ Network model saved: {path}\")\n",
    "        saved_models.append('isolation_forest_network')\n",
    "    else:\n",
    "        print(f\"✗ Failed to save network model: {path}\")\n",
    "\n",
    "if 'isolation_forest_site' in models:\n",
    "    model = models['isolation_forest_site']\n",
    "    metadata = ModelMetadata(\n",
    "        model_name='isolation_forest_site',\n",
    "        version='1.0.0',\n",
    "        model_type='isolation_forest',\n",
    "        training_date=datetime.now().isoformat(),\n",
    "        description='Isolation Forest for detecting site metric anomalies',\n",
    "        hyperparameters={\n",
    "            'contamination': 0.1,\n",
    "            'n_estimators': 100,\n",
    "            'max_samples': 'auto',\n",
    "            'max_features': 1.0,\n",
    "            'random_state': 42\n",
    "        },\n",
    "        metrics=evaluation_results.get('site', {})\n",
    "    )\n",
    "    \n",
    "    success, path = model_manager.save_model(\n",
    "        model, 'isolation_forest_site', 'anomaly_detection', '1.0.0', metadata\n",
    "    )\n",
    "    \n",
    "    if success:\n",
    "        print(f\"✓ Site model saved: {path}\")\n",
    "        saved_models.append('isolation_forest_site')\n",
    "    else:\n",
    "        print(f\"✗ Failed to save site model: {path}\")\n",
    "\n",
    "if 'isolation_forest_link' in models:\n",
    "    model = models['isolation_forest_link']\n",
    "    metadata = ModelMetadata(\n",
    "        model_name='isolation_forest_link',\n",
    "        version='1.0.0',\n",
    "        model_type='isolation_forest',\n",
    "        training_date=datetime.now().isoformat(),\n",
    "        description='Isolation Forest for detecting link metric anomalies',\n",
    "        hyperparameters={\n",
    "            'contamination': 0.1,\n",
    "            'n_estimators': 100,\n",
    "            'max_samples': 'auto',\n",
    "            'max_features': 1.0,\n",
    "            'random_state': 42\n",
    "        },\n",
    "        metrics=evaluation_results.get('link', {})\n",
    "    )\n",
    "    \n",
    "    success, path = model_manager.save_model(\n",
    "        model, 'isolation_forest_link', 'anomaly_detection', '1.0.0', metadata\n",
    "    )\n",
    "    \n",
    "    if success:\n",
    "        print(f\"✓ Link model saved: {path}\")\n",
    "        saved_models.append('isolation_forest_link')\n",
    "    else:\n",
    "        print(f\"✗ Failed to save link model: {path}\")\n",
    "\n",
    "print(f\"\\n{len(saved_models)} models saved successfully!\")\n",
    "\n",
    "# Also save scalers separately for reference\n",
    "scalers_data = {\n",
    "    'network': {'mean': scalers['network'].mean_.tolist(),\n",
    "                'scale': scalers['network'].scale_.tolist()} if 'network' in scalers else None,\n",
    "    'site': {'mean': scalers['site'].mean_.tolist(),\n",
    "             'scale': scalers['site'].scale_.tolist()} if 'site' in scalers else None,\n",
    "    'link': {'mean': scalers['link'].mean_.tolist(),\n",
    "             'scale': scalers['link'].scale_.tolist()} if 'link' in scalers else None,\n",
    "}\n",
    "\n",
    "scalers_path = MODELS_DIR / 'scalers_metadata.json'\n",
    "with open(scalers_path, 'w') as f:\n",
    "    json.dump(scalers_data, f, indent=2)\n",
    "print(f\"✓ Scalers metadata saved: {scalers_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d5571f",
   "metadata": {},
   "source": [
    "## Test Serialized Models\n",
    "\n",
    "Load the pickled models and verify they work correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc883d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing serialized models...\\n\")\n",
    "\n",
    "# List available models\n",
    "available_models = model_manager.list_models('anomaly_detection')\n",
    "print(\"Available models:\")\n",
    "for model_name, versions in available_models.items():\n",
    "    print(f\"  {model_name}: {versions}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOADING AND TESTING MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load and test network model\n",
    "print(\"\\n1. Testing Network Anomaly Detector:\")\n",
    "loaded_model, loaded_metadata = model_manager.load_model(\n",
    "    'isolation_forest_network', 'anomaly_detection', '1.0.0'\n",
    ")\n",
    "\n",
    "if loaded_model is not None:\n",
    "    # Test prediction\n",
    "    test_input = network_features.iloc[:5].values\n",
    "    scaled_input = scalers['network'].transform(test_input)\n",
    "    predictions = loaded_model.predict(scaled_input)\n",
    "    scores = loaded_model.score_samples(scaled_input)\n",
    "    \n",
    "    print(f\"  ✓ Model loaded successfully\")\n",
    "    print(f\"  ✓ Test predictions: {predictions[:3]}\")\n",
    "    print(f\"  ✓ Anomaly scores: {scores[:3]}\")\n",
    "    \n",
    "    if loaded_metadata:\n",
    "        print(f\"  ✓ Metadata: v{loaded_metadata.version} - {loaded_metadata.description}\")\n",
    "else:\n",
    "    print(f\"  ✗ Failed to load model\")\n",
    "\n",
    "# Load and test site model\n",
    "print(\"\\n2. Testing Site Anomaly Detector:\")\n",
    "loaded_model, loaded_metadata = model_manager.load_model(\n",
    "    'isolation_forest_site', 'anomaly_detection', '1.0.0'\n",
    ")\n",
    "\n",
    "if loaded_model is not None:\n",
    "    test_input = site_features.iloc[:5].values\n",
    "    scaled_input = scalers['site'].transform(test_input)\n",
    "    predictions = loaded_model.predict(scaled_input)\n",
    "    scores = loaded_model.score_samples(scaled_input)\n",
    "    \n",
    "    print(f\"  ✓ Model loaded successfully\")\n",
    "    print(f\"  ✓ Test predictions: {predictions[:3]}\")\n",
    "    print(f\"  ✓ Anomaly scores: {scores[:3]}\")\n",
    "    \n",
    "    if loaded_metadata:\n",
    "        print(f\"  ✓ Metadata: v{loaded_metadata.version} - {loaded_metadata.description}\")\n",
    "else:\n",
    "    print(f\"  ✗ Failed to load model\")\n",
    "\n",
    "# Load and test link model\n",
    "print(\"\\n3. Testing Link Anomaly Detector:\")\n",
    "loaded_model, loaded_metadata = model_manager.load_model(\n",
    "    'isolation_forest_link', 'anomaly_detection', '1.0.0'\n",
    ")\n",
    "\n",
    "if loaded_model is not None:\n",
    "    test_input = link_features.iloc[:5].values\n",
    "    scaled_input = scalers['link'].transform(test_input)\n",
    "    predictions = loaded_model.predict(scaled_input)\n",
    "    scores = loaded_model.score_samples(scaled_input)\n",
    "    \n",
    "    print(f\"  ✓ Model loaded successfully\")\n",
    "    print(f\"  ✓ Test predictions: {predictions[:3]}\")\n",
    "    print(f\"  ✓ Anomaly scores: {scores[:3]}\")\n",
    "    \n",
    "    if loaded_metadata:\n",
    "        print(f\"  ✓ Metadata: v{loaded_metadata.version} - {loaded_metadata.description}\")\n",
    "else:\n",
    "    print(f\"  ✗ Failed to load model\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL TESTS COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nModels are ready for deployment in the web application.\")\n",
    "print(f\"Model location: {MODELS_DIR}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
